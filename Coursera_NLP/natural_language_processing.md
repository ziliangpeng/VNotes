# Natural Language Processing

## Language Modelling


* Treats sentence as [Markov Chain]\(generated by [Markov Process]), in order to model probability of each sentence.

	* Trigram Model: P( wi | wi-1, wi-2 )
	* Bigram Model: P( wi | wi-1 )
	* Unigram Model: P( wi )
	* Trigram model gives very good result, very difficult to improve upon(Four-gram, Five-gram only gives very minor improvement).

[Markov Chain]: http://en.wikipedia.org/wiki/Markov_chain
[Markov Process]: http://en.wikipedia.org/wiki/Markov_process

* [Perplexity]: Evaluating the model

[Perplexity]: http://en.wikipedia.org/wiki/Perplexity

* [Linear Interpolation]: combine Trigram, Bigram and Unigram, given each a weight.

[Linear Interpolation]: http://en.wikipedia.org/wiki/Linear_interpolation

* Discounting Methods: slight reduce p of known words(reduce u from count where 0 < u < 1).


## Tagging & [HMM]

[HMM]:http://en.wikipedia.org/wiki/Hidden_Markov_model

* Problem: Given word sequence: X1, X2, X3, …, Xn, tag the sequence as Y1, Y2, Y3, …, Yn.
	* construct sequence X1, X2, …, Xn, Y1, Y2, …, Yn
	* give X1, X2, …, Xn, find max Y for P(X1, X2, …, Xn, Y1, Y2, …, Yn)
	
* [Viterbi Algorithm]: Brute force search has complexity of 'number of tag ^ length of sequence'. <br> 
  Viterbi algorithm is a DP approach to find the sequence that gives max Y.
  
[Viterbi Algorithm]: http://en.wikipedia.org/wiki/Viterbi_algorithm


## 3